# CS121 Linear regression
#
# SHEENA CHU (sheenachu)

import numpy as np
from asserts import assert_Xy, assert_Xbeta


#############################
#                           #
#  Our code: DO NOT MODIFY  #
#                           #
#############################


def prepend_ones_column(A):
    '''
    Add a ones column to the left side of an array

    Inputs: 
        A: a numpy array

    Output: a numpy array
    '''
    ones_col = np.ones((A.shape[0], 1))
    return np.hstack([ones_col, A])


def linear_regression(X, y):
    '''
    Compute linear regression. Finds model, beta, that minimizes
    X*beta - Y in a least squared sense.

    Accepts inputs with type array
    Returns beta, which is used only by apply_beta

    Examples
    --------
    >>> X = np.array([[5, 2], [3, 2], [6, 2.1], [7, 3]]) # predictors
    >>> y = np.array([5, 2, 6, 6]) # dependent
    >>> beta = linear_regression(X, y)  # compute the coefficients
    >>> beta
    array([ 1.20104895,  1.41083916, -1.6958042 ])
    >>> apply_beta(beta, X) # apply the function defined by beta
    array([ 4.86363636,  2.04195804,  6.1048951 ,  5.98951049])
    '''
    assert_Xy(X, y, fname='linear_regression')

    X_with_ones = prepend_ones_column(X)

    # Do actual computation
    beta = np.linalg.lstsq(X_with_ones, y)[0]

    return beta


def apply_beta(beta, X):
    '''
    Apply beta, the function generated by linear_regression, to the
    specified values

    Inputs:
        model: beta as returned by linear_regression
        Xs: 2D array of floats

    Returns:
        result of applying beta to the data, as an array.

        Given:
            beta = array([B0, B1, B2,...BK])
            Xs = array([[x11, x12, ..., x0K],
                        [x21, x22, ..., x1K],
                        ...
                        [xN1, xN2, ..., xNK]])

            result will be:
            array([B0+B1*x11+B2*x12+...+BK*x1K,
                   B0+B1*x21+B2*x22+...+BK*x2K,
                   ...
                   B0+B1*xN1+B2*xN2+...+BK*xNK])
    '''
    assert_Xbeta(X, beta, fname='apply_beta')

    # Add a column of ones
    X_incl_ones = prepend_ones_column(X)

    # Calculate X*beta
    yhat = np.dot(X_incl_ones, beta)
    return yhat


def read_file(filename):
    '''
    Read data from the specified file.  Split the lines and convert
    float strings into floats.  Assumes the first row contains labels
    for the columns.

    Inputs:
      filename: name of the file to be read

    Returns:
      (list of strings, 2D array)
    '''
    with open(filename) as f:
        labels = f.readline().strip().split(',')
        data = np.loadtxt(f, delimiter=',', dtype=np.float64)
        return labels, data


###############
#             #
#  Your code  #
#             #
###############

#----------------------------------
# Helper Functions
#----------------------------------

def create_model(filename, dependent, predictors, training = True):
    '''
    Function creates a dicitonary

    Inputs:
            filename: string
            dependent: index of the dependent variable
            predictors: indecies of the predictor variables
            training: Boolean

    Returns:
            Model: dictionary
    '''

    model = {}
    if training:
        labels, data = read_file('data/{}/training.csv'.format(filename))
    else:
        labels, data = read_file('data/{}/testing.csv'.format(filename))
    model.update({'name' : filename})
    model.update({'labels' : labels})
    model.update({'depend' : dependent})
    model.update({'predict' : predictors})
    model.update({'X' : data[:,min(predictors):(max(predictors)+1)]})
    model.update({'y' : data[:,dependent]})
    return model

def r_squared(y,yhat):
    '''
    Function calculates R-squared value given y and yhat

    Inputs:
            y: 1D array
            yhat: 1D array
    Output:
            r2: float
    '''
    r2 = 1 - (np.sum((y-yhat)**2) / np.sum((y-y.mean())**2))
    return r2

def calc_r_squared(X, y, X2 = None, y2 = None):
    '''
    Function calculates R-squared for a given dataset.
    X2 and y2 are used when calculating R-squared for
    testing data using a model constructed training dataset.

    Inputs:
            X: 2D array
            y: 1D array
            X2: 2D array
            2y: 1D array

    Output:
            r2: float
    '''
    beta = linear_regression(X, y)
    
    if type(X2) == type(None) or type(y2) == type(None):
        yhat = apply_beta(beta, X)
        r2 = r_squared(y, yhat)
    else:
        yhat = apply_beta(beta, X2)
        r2 = r_squared(y2, yhat)
    
    return r2

def sim_k(X, y, predict, mod_index):
    '''
    Function simulates the Kth step of creating a K-variable model.
    Returns the highest R-squared value and index of the variable
    to be added to the model.

    Inputs:
            X: 2D array
            y: 1D array
            redict: list of predictor indices
            mod_index: list of indices of predictor variables
                    included in the model

    Returns:
            r2: float
            index: integer
    '''

    r2 = 0.0
    index = 0
    for i in predict:
        if i not in mod_index:
            r2_2 = calc_r_squared(X[:,[i] + mod_index], y)
            if r2_2 > r2:
                r2 = r2_2
                index = i
    return r2, index

#------------------------
# Task 1
#------------------------

def task_1(model):
    '''
    Function completes task 1.
    Prints a table with R-squared values for how well each predictor variable
    separately predicts the dependent variable.
    R-squared is calculated for each individual predictor variable in Task 1a.
    R-squared is calculated for all predictor variables in Task 1b.

    Inputs:
            model: dictionary
    '''

    X = model['X']
    y = model['y']
    print('{} Task 1a:'.format(model['name'].upper()))
    for i in model['predict']:
        r2 = calc_r_squared(X[:,[i]], y)
        print('{}: {:.2f}'.format(model['labels'][i], r2))

    print('{} Task 1b:'.format(model['name'].upper()))
    r2 = calc_r_squared(X, y)
    print('{} R2: {:.2f}'.format(', '.join(model['labels']
            [min(model['predict']):(max(model['predict']) + 1)]), r2))

#------------------------
# Task 2
#------------------------

def task_2(model):
    '''
    Function completes task 2.
    Prints the names of the two predictors that yield
    the bivariate model with the highest R-squared value,
    along with its R-squared value.

    Inputs:
            model: dictionary
    '''
    X = model['X']
    y = model['y']
    r2 = 0
    p = []

    for i in model['predict']:
        for j in model['predict']:
            if i != j:
                r2_2 = calc_r_squared(X[:,[i, j]], y)
                if r2_2 >= r2:
                    r2 = r2_2
                    p = [model['labels'][i], model['labels'][j]]
    print('{} Task 2:'.format(model['name'].upper()))
    print('{} R2: {:.2f}'.format(', '.join(p), r2))

#----------------------------------
# Task 3
#----------------------------------

def task_3a(model):
    '''
    Function completes Task 3a.
    N = number of predictor variables in your data set.
    Prints a table that lists K, the predictor variables in the best K-variable
    model chosen by the forward selection algorithm,
    and R - squared for this model for 1≤K≤N.


    Inputs:
            model: dictionary
    '''

    X = model['X']
    y = model['y']
    N = len(model['predict'])
    mod_index = []

    print('{} Task 3a:'.format(model['name'].upper()))
    while len(mod_index) < N:
        r2, index = sim_k(X, y, model['predict'], mod_index)
        mod_index.append(index)
        print('{} R2: {:.2f}'.format(', '.join([model['labels'][i] for i in mod_index]), r2))

def task_3b(model, threshold):
    '''
    Function completes Task 3b.
    N = number of predictor variables in your data set.
    Prints the model's R-squared value and the names of the selected predictors
    for the K-1 variable model in which the improvement in R-squared
    between the model with K variables and the model with K-1 variables
    is strictly less than a specified threshold.

    Inputs:
            model: dicitonary
            threshold: float
    '''

    X = model['X']
    y = model['y']

    mod_index = []    
    old_r2 = -(threshold + 1)
    r2 = 0

    print('{} Task 3b:'.format(model['name'].upper()))
    while (r2 - old_r2) > threshold:
        old_r2 = r2
        r2, index = sim_k(X, y, model['predict'], mod_index)
        mod_index.append(index)
    mod_index.pop()
    print('Threshold {}:'.format(threshold))
    print('{} R2: {:.2f}'.format(', '.join([model['labels'][i] for i in mod_index]), old_r2))

#----------------------------------
# Task 4
#----------------------------------

def task_4(training_model, testing_model):
    '''
    Function completes Task 4.
    N = number of predictor variables in dataset.
    Prints table that lists K, the predictor variables in the best
    K-variable model chosen by the forward selection algorithm
    using the training data, and R-squared for this using the model
    on the testing data for 1≤K≤N.

    Inputs:
            training_model: dicitonary
            testing_model: dictionary
    '''

    X1 = training_model['X']
    y1 = training_model['y']
    X2 = testing_model['X']
    y2 = testing_model['y']
    N = len(training_model['predict'])
    mod_index = []

    print('{} Task 4:'.format(testing_model['name'].upper()))
    while len(mod_index) < N:
        r2_train, index = sim_k(X1, y1, training_model['predict'], 
                            mod_index)
        mod_index.append(index)
        r2 = calc_r_squared(X1[:,mod_index], y1, X2[:,mod_index], y2)
        print('{} R2: {:.2f}'.format(', '.join([testing_model['labels'][i] for i in mod_index]), r2))